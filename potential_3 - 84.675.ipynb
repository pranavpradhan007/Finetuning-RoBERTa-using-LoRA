{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7bce8e1-04d2-4cd3-9071-f26b1dd6ea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n",
      "\n",
      "--- Loading Tokenizer and Dataset ---\n",
      "\n",
      "--- Preprocessing Dataset ---\n",
      "Number of labels: 4\n",
      "Labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "Example tokenized entry: {'labels': 2, 'input_ids': [0, 28216, 312, 4, 6033, 44121, 3727, 20693, 5, 1378, 36, 1251, 43, 1201, 111, 7787, 12, 5727, 268, 6, 2298, 852, 18, 25564, 37457, 9484, 9, 9620, 12, 4469, 282, 2857, 6, 32, 1782, 2272, 456, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "--- Splitting Data (Train/Eval) ---\n",
      "Train dataset size: 114000\n",
      "Eval dataset size: 6000\n",
      "\n",
      "--- Loading Base RoBERTa Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded. Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import evaluate  # Use the evaluate library for metrics\n",
    "import time\n",
    "from tqdm.notebook import tqdm # Use notebook tqdm for better colab rendering\n",
    "\n",
    "from transformers import (\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL = 'roberta-base'\n",
    "DATASET_NAME = 'ag_news'\n",
    "OUTPUT_DIR = \"results_lora_agnews\"\n",
    "SEED = 42\n",
    "MAX_TRAINABLE_PARAMS = 1_000_000 # Project constraint\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# --- 1. Load Tokenizer and Dataset ---\n",
    "print(\"\\n--- Loading Tokenizer and Dataset ---\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BASE_MODEL)\n",
    "dataset = load_dataset(DATASET_NAME, split='train')\n",
    "\n",
    "# --- 2. Preprocess Data ---\n",
    "print(\"\\n--- Preprocessing Dataset ---\")\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize, truncate long sequences, pad shorter sequences\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512) # Using max_length for consistency\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Extract class info\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "label2id = {label: i for i, label in id2label.items()} # Useful for the model config\n",
    "\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Labels: {class_names}\")\n",
    "print(f\"Example tokenized entry: {tokenized_dataset[0]}\")\n",
    "\n",
    "# --- 3. Split Data ---\n",
    "print(\"\\n--- Splitting Data (Train/Eval) ---\")\n",
    "# Using a slightly larger eval set might give more stable results\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=0.05, seed=SEED, stratify_by_column=\"labels\") # 5% for eval\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# --- 4. Data Collator ---\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# --- 5. Load Base Model ---\n",
    "print(\"\\n--- Loading Base RoBERTa Model ---\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Freeze base model parameters (standard practice for PEFT methods like LoRA)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Base model loaded. Device: {model.device}\") # Should be CPU initially\n",
    "# print(model) # Uncomment to see model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae22f17-278b-490b-9ccc-534327a8671a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6e0eb-6bed-41b5-bde1-7361ca70c763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3e04e99-8d87-4577-8740-7e981437af85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating parameters on a temporary model instance...\n",
      "Chosen LoRA Config:\n",
      "  r = 8\n",
      "  alpha = 32\n",
      "  dropout = 0.1\n",
      "  target_modules = ['query', 'value', 'roberta.encoder.layer.*.output.dense']\n",
      "  bias = lora_only\n",
      "Calculated Trainable Parameters: 907,012\n",
      "Parameter count is within the limit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6. LoRA Configuration ---\n",
    "# Utility to calculate trainable params for a given config\n",
    "def calculate_trainable_parameters(model, peft_config):\n",
    "    # Create a temporary PEFT model instance to calculate params\n",
    "    # Avoids modifying the main model object prematurely\n",
    "    temp_peft_model = get_peft_model(model, peft_config)\n",
    "    # get_nb_trainable_parameters returns a tuple: (trainable, total)\n",
    "    trainable_params, total_params = temp_peft_model.get_nb_trainable_parameters()\n",
    "    del temp_peft_model # Clean up the temporary model object\n",
    "    return trainable_params # <<< FIX: Return only the trainable count (integer)\n",
    "\n",
    "# --- Experiment with LoRA settings here ---\n",
    "# Goal: Get close to MAX_TRAINABLE_PARAMS without exceeding it.\n",
    "# Common modules to target in RoBERTa: ['query', 'value'] in self-attention\n",
    "# Other possibilities: ['key', 'dense'] in attention output, ['dense'] in intermediate MLP\n",
    "# Start with r=8 or r=16 and target_modules=['query', 'value']\n",
    "\n",
    "lora_r = 8  # Rank\n",
    "lora_alpha = 32 # Alpha (scaling factor, often 2*r)\n",
    "lora_dropout = 0.1\n",
    "# Target both query and value matrices in self-attention layers\n",
    "# target_modules = [\"query\", \"value\"] # Good starting point\n",
    "# target_modules = [\"query\", \"key\", \"value\", \"dense\"] # More params\n",
    "target_modules = [\"query\", \"value\", \"roberta.encoder.layer.*.output.dense\"] # Example targeting specific layers\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"lora_only\",  # Or 'all' or 'lora_only', 'none' saves params\n",
    "    task_type=TaskType.SEQ_CLS # Important for sequence classification\n",
    ")\n",
    "\n",
    "# Calculate parameters *before* creating the final PEFT model\n",
    "# Need to load a temporary base model instance as get_peft_model modifies in place if model is already on GPU?\n",
    "# Let's try directly on the CPU model first.\n",
    "print(\"Calculating parameters on a temporary model instance...\")\n",
    "temp_base_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")\n",
    "# Ensure the temporary model is configured before passing to the calculation function\n",
    "num_trainable = calculate_trainable_parameters(temp_base_model, peft_config)\n",
    "del temp_base_model # Free memory\n",
    "\n",
    "print(f\"Chosen LoRA Config:\")\n",
    "print(f\"  r = {lora_r}\")\n",
    "print(f\"  alpha = {lora_alpha}\")\n",
    "print(f\"  dropout = {lora_dropout}\")\n",
    "print(f\"  target_modules = {target_modules}\")\n",
    "print(f\"  bias = {peft_config.bias}\")\n",
    "# This print statement should now work correctly\n",
    "print(f\"Calculated Trainable Parameters: {num_trainable:,}\") # Now num_trainable is an int\n",
    "\n",
    "if num_trainable > MAX_TRAINABLE_PARAMS:\n",
    "    raise ValueError(f\"Trainable parameters ({num_trainable:,}) exceed the limit ({MAX_TRAINABLE_PARAMS:,}). Adjust LoRA config (e.g., lower 'r', fewer 'target_modules').\")\n",
    "elif num_trainable == 0:\n",
    "     raise ValueError(f\"Trainable parameters is zero. Check LoRA config (e.g., 'target_modules'). Valid modules often include 'query', 'value', 'key', 'dense'.\")\n",
    "else:\n",
    "    print(\"Parameter count is within the limit.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3e0bed9-1483-4891-ae03-8467bb53b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating PEFT Model ---\n",
      "trainable params: 907,012 || all params: 125,537,288 || trainable%: 0.7225\n",
      "\n",
      "--- Setting up Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmg9725/.local/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/rmg9725/.local/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/home/rmg9725/.local/lib/python3.9/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_5559/2353975628.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71250' max='71250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71250/71250 1:52:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.204269</td>\n",
       "      <td>0.932333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.200650</td>\n",
       "      <td>0.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.190489</td>\n",
       "      <td>0.940500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.178691</td>\n",
       "      <td>0.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.197201</td>\n",
       "      <td>0.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>0.183170</td>\n",
       "      <td>0.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.185998</td>\n",
       "      <td>0.946333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.126800</td>\n",
       "      <td>0.198615</td>\n",
       "      <td>0.946167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.176600</td>\n",
       "      <td>0.200388</td>\n",
       "      <td>0.947167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Finished ---\n",
      "Training Time: 112.08 minutes\n",
      "***** train metrics *****\n",
      "  epoch                    =        10.0\n",
      "  total_flos               = 282250221GF\n",
      "  train_loss               =      0.1623\n",
      "  train_runtime            =  1:52:04.36\n",
      "  train_samples_per_second =     169.533\n",
      "  train_steps_per_second   =      10.596\n",
      "Training metrics saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- 7. Create PEFT Model ---\n",
    "print(\"\\n--- Creating PEFT Model ---\")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# --- 8. Training Setup ---\n",
    "print(\"\\n--- Setting up Training ---\")\n",
    "\n",
    "# Metrics Calculation Function\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "    }\n",
    "\n",
    "# Training Arguments\n",
    "# Adjust hyperparameters based on experiments\n",
    "# Common LoRA learning rates: 5e-5, 1e-4, 2e-4\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=10, # Train for a few epochs\n",
    "    per_device_train_batch_size=16, # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_ratio=0.1, # Warmup for 10% of steps\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-4, # Starting point for LoRA\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "    save_total_limit=2, # Keep only the last 2 checkpoints\n",
    "    load_best_model_at_end=True, # Load the best model based on eval metric\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\", # Disable external reporting (like wandb) for simplicity\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if GPU available\n",
    "    gradient_checkpointing=False, # Set to True if memory is tight, but slows down training\n",
    "    seed=SEED,\n",
    "    # optim=\"adamw_torch\", # Default AdamW from PyTorch\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model, # Use the PEFT model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 9. Start Training ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "start_time = time.time()\n",
    "train_result = trainer.train()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Training Time: {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# Log metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "print(\"Training metrics saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0d81e-2967-4bdf-b621-50c85c07e3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9723567-0342-4fc5-a425-9dad6ed59e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7651440-623b-4586-afa3-8d7b2c8d3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Best Model on Evaluation Set ---\n",
      "Confirming trainable parameters of the final loaded model:\n",
      "trainable params: 907,012 || all params: 125,537,288 || trainable%: 0.7225\n",
      "\n",
      "Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_accuracy           =     0.9483\n",
      "  eval_loss               =     0.1933\n",
      "  eval_runtime            = 0:00:09.92\n",
      "  eval_samples_per_second =    604.463\n",
      "  eval_steps_per_second   =       9.47\n",
      "Evaluation Metrics: {'eval_loss': 0.19326800107955933, 'eval_accuracy': 0.9483333333333334, 'eval_runtime': 9.9262, 'eval_samples_per_second': 604.463, 'eval_steps_per_second': 9.47, 'epoch': 10.0}\n",
      "--------------------------------------------------\n",
      "Final Evaluation Accuracy: 0.9483\n",
      "Final Model Trainable Parameters: 907,012\n",
      "--------------------------------------------------\n",
      "Final model parameter count is within the limit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 10. Evaluate Final Model on Eval Set ---\n",
    "print(\"\\n--- Evaluating Best Model on Evaluation Set ---\")\n",
    "\n",
    "# The trainer automatically loads the best model if load_best_model_at_end=True\n",
    "# Let's explicitly print the trainable parameters of the model loaded by the trainer\n",
    "print(\"Confirming trainable parameters of the final loaded model:\")\n",
    "# Ensure the model loaded by the trainer is the PEFT model\n",
    "if hasattr(trainer.model, 'print_trainable_parameters'):\n",
    "    trainer.model.print_trainable_parameters()\n",
    "else:\n",
    "    # This case shouldn't happen if setup is correct, but good to check\n",
    "    total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "    print(f\"Model is not a PeftModel? Total params: {total_params:,}, Trainable params: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "print(\"\\nRunning final evaluation...\")\n",
    "eval_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "# Log and save evaluation metrics\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "print(f\"Evaluation Metrics: {eval_metrics}\")\n",
    "\n",
    "final_accuracy = eval_metrics.get(\"eval_accuracy\", 0)\n",
    "final_trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad) # Re-calculate just to be sure\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final Evaluation Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"Final Model Trainable Parameters: {final_trainable_params:,}\") # Print the final count clearly\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Double-check against the limit\n",
    "if final_trainable_params > MAX_TRAINABLE_PARAMS:\n",
    "     print(f\"WARNING: Final model trainable parameters ({final_trainable_params:,}) exceed the limit ({MAX_TRAINABLE_PARAMS:,})!\")\n",
    "elif final_trainable_params == 0:\n",
    "     print(f\"WARNING: Final model has 0 trainable parameters!\")\n",
    "else:\n",
    "     print(\"Final model parameter count is within the limit.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4598b-def3-4249-90fe-dcd21cad16e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf81278-943e-4aaf-9456-aedb85450ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "801158fa-5dee-4c93-b64c-0135e2476ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Inference on Unlabelled Test Data ---\n",
      "Loading test dataset from: test_unlabelled.pkl\n",
      "Test dataset loaded successfully. Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset features: ['text']\n",
      "Number of examples: 8000\n",
      "Preprocessing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3f9eb5e3204ed8a3c7e58cfd55296b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 8000 examples using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:12<00:00,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Generated 8000 predictions.\n",
      "Generating sequential IDs as 'ID' column was not found in the original dataset.\n",
      "Predictions saved to results_lora_agnews/inference_output.csv\n",
      "\n",
      "--- Project Notebook Finished ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 12. Run Inference on Unlabelled Test Data ---\n",
    "print(\"\\n--- Running Inference on Unlabelled Test Data ---\")\n",
    "\n",
    "from tqdm import tqdm # Standard tqdm\n",
    "from torch.utils.data import DataLoader # Ensure DataLoader is imported\n",
    "\n",
    "def predict_on_test_dataset(model, tokenizer, test_dataset_path=\"test_unlabelled.pkl\", output_directory=OUTPUT_DIR, batch_size=32):\n",
    "    \"\"\"Run inference on the test dataset and save predictions to CSV\"\"\"\n",
    "    \n",
    "    # Use the global OUTPUT_DIR defined earlier for consistency\n",
    "    output_dir = output_directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading test dataset from: {test_dataset_path}\")\n",
    "    if not os.path.exists(test_dataset_path):\n",
    "        print(f\"ERROR: Test dataset file not found at {test_dataset_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load the test dataset - Assuming it's a datasets.Dataset pickled\n",
    "    try:\n",
    "        with open(test_dataset_path, 'rb') as f:\n",
    "            test_dataset = pickle.load(f)\n",
    "        # Verify it's a Dataset object\n",
    "        if not isinstance(test_dataset, Dataset):\n",
    "             print(f\"Warning: Loaded object is type {type(test_dataset)}, not datasets.Dataset. Trying to convert from Pandas DataFrame.\")\n",
    "             # If it was saved as a DataFrame:\n",
    "             if isinstance(test_dataset, pd.DataFrame):\n",
    "                 if 'text' not in test_dataset.columns:\n",
    "                     raise ValueError(\"Loaded DataFrame does not contain a 'text' column.\")\n",
    "                 test_dataset = Dataset.from_pandas(test_dataset)\n",
    "             else:\n",
    "                 raise TypeError(\"Loaded pickle file is not a datasets.Dataset or pandas.DataFrame.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing pickled dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Test dataset loaded successfully. Type: {type(test_dataset)}\")\n",
    "    print(f\"Dataset features: {test_dataset.column_names}\")\n",
    "    print(f\"Number of examples: {len(test_dataset)}\")\n",
    "\n",
    "    if 'text' not in test_dataset.column_names:\n",
    "        print(\"ERROR: 'text' column not found in the loaded test dataset.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Preprocessing test data...\")\n",
    "    # Tokenize the text data\n",
    "    def preprocess(examples):\n",
    "        # Ensure padding is consistent with training, e.g., max_length\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    # Important: Remove the 'text' column after tokenization\n",
    "    # Keep other columns needed for ID mapping if they exist (e.g., 'ID')\n",
    "    columns_to_remove = ['text']\n",
    "    test_tokenized = test_dataset.map(preprocess, batched=True, remove_columns=columns_to_remove)\n",
    "\n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    # Create data loader for batched inference\n",
    "    # Use the same data_collator as training for consistency\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "    test_dataloader = DataLoader(test_tokenized, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "    all_predictions = []\n",
    "    print(f\"Running inference on {len(test_dataset)} examples using device: {device}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Inference\"):\n",
    "            # Move batch to device, ensure only expected inputs are passed\n",
    "            batch_inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "            if not batch_inputs:\n",
    "                print(\"Warning: No model inputs found in batch. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            outputs = model(**batch_inputs)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            all_predictions.append(predictions.cpu().numpy()) # Move predictions to CPU before converting to numpy\n",
    "\n",
    "    # Concatenate predictions from all batches\n",
    "    if not all_predictions:\n",
    "        print(\"ERROR: No predictions were generated.\")\n",
    "        return None\n",
    "\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    print(f\"Inference completed. Generated {len(all_predictions)} predictions.\")\n",
    "\n",
    "    # Create output DataFrame\n",
    "    # Check if the original dataset had an 'ID' column, otherwise generate sequential IDs\n",
    "    if 'ID' in test_dataset.column_names:\n",
    "         ids = test_dataset['ID']\n",
    "         if len(ids) != len(all_predictions):\n",
    "              print(f\"Warning: Length mismatch between original IDs ({len(ids)}) and predictions ({len(all_predictions)}). Generating sequential IDs.\")\n",
    "              ids = range(len(all_predictions))\n",
    "         else:\n",
    "              print(\"Using 'ID' column from the original dataset.\")\n",
    "    else:\n",
    "         print(\"Generating sequential IDs as 'ID' column was not found in the original dataset.\")\n",
    "         ids = range(len(all_predictions))\n",
    "\n",
    "    output_df = pd.DataFrame({'ID': ids, 'Label': all_predictions})\n",
    "\n",
    "    # Save to CSV\n",
    "    submission_filename = os.path.join(output_dir, \"inference_output.csv\")\n",
    "    output_df.to_csv(submission_filename, index=False)\n",
    "    print(f\"Predictions saved to {submission_filename}\")\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "# === Call the Prediction Function ===\n",
    "# Use trainer.model because it holds the best model loaded after training\n",
    "# Pass the OUTPUT_DIR defined earlier in the notebook\n",
    "predictions = predict_on_test_dataset(\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_dataset_path=\"test_unlabelled.pkl\", # Make sure this path is correct\n",
    "    output_directory=OUTPUT_DIR,\n",
    "    batch_size=training_args.per_device_eval_batch_size # Use eval batch size from training args\n",
    ")\n",
    "\n",
    "print(\"\\n--- Project Notebook Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06321f-13f5-4057-b762-c06db69a3104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
